{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do it! 딥러닝 입문 \n",
    "# 7장 여러개를 구분합니다- 다중 분류\n",
    "![](https://github.com/Asunny2019/Deeplearning_from_scratch/blob/main/Deeplearning_from_scratch1/img/ch7_1.JPG?raw=true)\n",
    "\n",
    "- 지금까지 이진분류하는 단층의 딥러닝 모델을 만들었다. 이제부터 여러개의 이미지를 분류해내는 다층 신경망을 만들고자 한다.\n",
    "\n",
    "- 이진분류 신경망은 출력층의 뉴런이 1개였다면(양성 클래스), 다중분류 신경망의 경우는 만일 3개로 이미지를 분류해내고자 한다면(강아지, 고양이, 새) 출력층이 3개가 필요하다. __구분해내려고 하는 클래스의 개수가 출력층의 뉴런의 개수__ 가 된다. \n",
    "\n",
    "![](https://github.com/Asunny2019/Deeplearning_from_scratch/blob/main/Deeplearning_from_scratch1/img/ch7_2.JPG?raw=true)\n",
    "- 만일 다중분류를 할때 시그모이드 함수를 쓴다면,  \n",
    "    a. 출력층에서 각 노드에서 출력되는 값을 다 더하면 1보다 작거나 클 경우가 많다.  \n",
    "    b. 각 값들의 차이가 크지 않아서 클래스를 구분해내기 어렵다. \n",
    "    \n",
    "- 그래서 다중분류를 할때는 __소프트맥스(softmax)함수__ 를 사용해서 분류해낸다.\n",
    "\n",
    "## 소프트맥스(softmax)함수\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbcNHRR%2FbtqBPXmTZBh%2FOjPe56tF5LWmQChuKZgPa0%2Fimg.png)\n",
    "$$\\sigma_{i}(a)=\\frac{e^{a_{i}}}{\\sum_{j}e^{a_{j}}}$$\n",
    "\n",
    "\n",
    "![](https://github.com/Asunny2019/Deeplearning_from_scratch/blob/main/Deeplearning_from_scratch1/img/ch7_3.JPG?raw=true)\n",
    "- 왼쪽 그림은 위의 식, 오른쪽의 그림은 아래의 식이다.\n",
    "- 앞장에서 그린 시그모이드 함수에 적용된 값을 역산한 값이 왼쪽 그림의 2.20, 1.39, 0.85이다. 즉, 네트워크에서 학습된 선형출력값이다. 이 값을 소프트맥스 함수를 적용하면 0.59, 0.26,0.15가 된다.\n",
    "- 소프트맥스는 각 클래스에 속할 확률을 개별적으로 계산하는게 아니라, 각 클래스에 속할 확률을 정규화해준다. (즉, __소프트맥스를 적용한 값을 모두 더하면 1__ 이 된다.) 그래서 소프트맥스를 적용한 값은 각 클래스일 확률로 말할 수 있다. \n",
    "\n",
    "## 다중분류를 위한 손실함수- 크로스 엔트로피 손실함수\n",
    "\n",
    "크로스 엔트로피 손실함수\n",
    "$$L=-\\sum^{c}_{c=1}y_{c}log({a_{c})=}-(y_{1}log(a_{1})+y_{2}log(a_{2})+\\dots +y_{c}log(a_{c}))=-1 \\times log(a_{y=1})$$\n",
    "\n",
    "\n",
    "로지스틱 손실함수(이진 크로스 엔트로피 손실함수)  \n",
    "\n",
    "$L=-(y_log(a)+(1-y)log(1-a) \\qquad y=\\begin{cases} -log(a), & \\mbox{양성인 경우}\\\\\n",
    "-log(1-a), & \\mbox{음성인 경우}\\end{cases}$\n",
    "- 크로스 엔트로피 손실함수는 각 클래스 별로 target($y_c$)과 소프트맥스 출력값 $a_c$에 로그를 취한 log($a_c$)를 곱한 값을 더한 것이다.  \n",
    "- c개의 클래스 중에서 target값은 정답인 값만 1이고 나머지는 0이 나올 것이다. 그러면 크로스 엔트로피는 정답인 클래스만 남게 된다. 이 값은 이진 크로스 엔트로피의 양성클래스일 경우의 값과 동일하다.\n",
    "\n",
    "![](https://github.com/Asunny2019/Deeplearning_from_scratch/blob/main/Deeplearning_from_scratch1/img/ch7_4.JPG?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잠깐 편도함수의 연쇄법칙을 정리하고 가자면....\n",
    "\n",
    "### 편도함수의 연쇄법칙\n",
    "$z=f(x,y)가\\ x,\\ y의\\ 미분가능한\\ 함수라고\\ 하고,\\ 이때\\ x=g(x,t)와\\ y=h(s,t)가\\ 모두\\ s,\\ t의\\ 미분가능한\\ 함수라고\\ 가정하자.\\ 그러면$\n",
    "$${\\partial z \\over \\partial s} ={\\partial z \\over \\partial x}{\\partial x \\over \\partial s}+{\\partial z \\over \\partial y}{\\partial y \\over \\partial s}$$\n",
    "$${\\partial z \\over \\partial t} ={\\partial z \\over \\partial x}{\\partial x \\over \\partial t}+{\\partial z \\over \\partial y}{\\partial y \\over \\partial t}$$\n",
    "![](https://github.com/Asunny2019/Deeplearning_from_scratch/blob/main/Deeplearning_from_scratch1/img/ch7_5.JPG?raw=true)\n",
    "\n",
    "- 이 유형의 연쇄법칙에는 세가지 유형의 변수가 있다. s,t는 독립변수 x,y는 중간변수, z는 종속변수이다. 이 연쇄법칙을 기억하기 위해서는 나무 그림을 그리는 것이 도움이 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ${\\partial L\\over\\partial a}$  \n",
    "\n",
    "$L=-(y_{1}log(a_{1})+y_{2}log(a_{2})+y_{3}log(a_{3})$  \n",
    "\n",
    "${\\partial L\\over\\partial a_{1}}=-{\\partial \\over\\partial a_{1}}(y_{1}log(a_{1})+y_{2}log(a_{2})+y_{3}log(a_{3})=$\n",
    "$-$\n",
    "${y_{1}} \\over{ a_{1}}$\n",
    "\n",
    "\n",
    "${\\partial L\\over\\partial a_{2}}=$-${y_{2}} \\over{ a_{2}}\\$\n",
    "\n",
    "${\\partial L\\over\\partial a_{3}}=$-${y_{3}} \\over{ a_{3}}\\$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/Asunny2019/Deeplearning_from_scratch/blob/main/Deeplearning_from_scratch1/img/ch7_6.JPG?raw=true)\n",
    "![](https://github.com/Asunny2019/Deeplearning_from_scratch/blob/main/Deeplearning_from_scratch1/img/ch7_7.JPG?raw=true)\n",
    "![](https://github.com/Asunny2019/Deeplearning_from_scratch/blob/main/Deeplearning_from_scratch1/img/ch7_8.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/Asunny2019/Deeplearning_from_scratch/blob/main/Deeplearning_from_scratch1/img/ch7_9.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class MultiClassNetwork:\n",
    "    \n",
    "    def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n",
    "        self.units = units         # 은닉층의 뉴런 개수\n",
    "        self.batch_size = batch_size     # 배치 크기\n",
    "        self.w1 = None             # 은닉층의 가중치\n",
    "        self.b1 = None             # 은닉층의 절편\n",
    "        self.w2 = None             # 출력층의 가중치\n",
    "        self.b2 = None             # 출력층의 절편\n",
    "        self.a1 = None             # 은닉층의 활성화 출력\n",
    "        self.losses = []           # 훈련 손실\n",
    "        self.val_losses = []       # 검증 손실\n",
    "        self.lr = learning_rate    # 학습률\n",
    "        self.l1 = l1               # L1 손실 하이퍼파라미터\n",
    "        self.l2 = l2               # L2 손실 하이퍼파라미터\n",
    "\n",
    "    def forpass(self, x):\n",
    "        z1 = np.dot(x, self.w1) + self.b1        # 첫 번째 층의 선형 식을 계산합니다\n",
    "        self.a1 = self.sigmoid(z1)               # 활성화 함수를 적용합니다\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2  # 두 번째 층의 선형 식을 계산합니다.\n",
    "        return z2\n",
    "\n",
    "    def backprop(self, x, err):\n",
    "        m = len(x)       # 샘플 개수\n",
    "        # 출력층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
    "        w2_grad = np.dot(self.a1.T, err) / m\n",
    "        b2_grad = np.sum(err) / m\n",
    "        # 시그모이드 함수까지 그래디언트를 계산합니다.\n",
    "        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)\n",
    "        # 은닉층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
    "        w1_grad = np.dot(x.T, err_to_hidden) / m\n",
    "        b1_grad = np.sum(err_to_hidden, axis=0) / m\n",
    "        return w1_grad, b1_grad, w2_grad, b2_grad\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -100, None)            # 안전한 np.exp() 계산을 위해\n",
    "        a = 1 / (1 + np.exp(-z))              # 시그모이드 계산\n",
    "        return a\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        # 소프트맥스 함수\n",
    "        z = np.clip(z, -100, None)            # 안전한 np.exp() 계산을 위해\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=1).reshape(-1, 1)\n",
    "    # 분모는 exp_z를 행방향으로 모두 더해주는 것.\n",
    "    # reshape함수에서 행에 -1을 입력하면, \n",
    "    #열의 값을 지정해주면 알아서 행의 값이 생성된다는 의미.\n",
    " \n",
    "    def init_weights(self, n_features, n_classes):\n",
    "        self.w1 = np.random.normal(0, 1, \n",
    "                                   (n_features, self.units))  # (특성 개수, 은닉층의 크기)\n",
    "        self.b1 = np.zeros(self.units)                        # 은닉층의 크기\n",
    "        self.w2 = np.random.normal(0, 1, \n",
    "                                   (self.units, n_classes))   \n",
    "        # (은닉층의 크기, 클래스 개수) 이진분류는 클래스개수가 아니라 1이었음.\n",
    "        self.b2 = np.zeros(n_classes)\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        np.random.seed(42)\n",
    "        self.init_weights(x.shape[1], y.shape[1])    # 은닉층과 출력층의 가중치를 초기화합니다.\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            print('.', end='')\n",
    "            # 제너레이터 함수에서 반환한 미니배치를 순환합니다.\n",
    "            for x_batch, y_batch in self.gen_batch(x, y):\n",
    "                a = self.training(x_batch, y_batch)\n",
    "                # 안전한 로그 계산을 위해 클리핑합니다.\n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "                loss += np.sum(-y_batch*np.log(a))\n",
    "            self.losses.append((loss + self.reg_loss()) / len(x))\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.update_val_loss(x_val, y_val)\n",
    "\n",
    "    # 미니배치 제너레이터 함수\n",
    "    def gen_batch(self, x, y):\n",
    "        length = len(x)\n",
    "        bins = length // self.batch_size # 미니배치 횟수\n",
    "        if length % self.batch_size:\n",
    "            bins += 1                    # 나누어 떨어지지 않을 때\n",
    "        indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞습니다.\n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size * i\n",
    "            end = self.batch_size * (i + 1)\n",
    "            yield x[start:end], y[start:end]   # batch_size만큼 슬라이싱하여 반환합니다.\n",
    "            \n",
    "    def training(self, x, y):\n",
    "        m = len(x)                # 샘플 개수를 저장합니다.\n",
    "        z = self.forpass(x)       # 정방향 계산을 수행합니다.\n",
    "        a = self.softmax(z)       # 활성화 함수를 적용합니다.\n",
    "        err = -(y - a)            # 오차를 계산합니다.\n",
    "        # 오차를 역전파하여 그래디언트를 계산합니다.\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        # 그래디언트에서 페널티 항의 미분 값을 뺍니다\n",
    "        w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m\n",
    "        w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m\n",
    "        # 은닉층의 가중치와 절편을 업데이트합니다.\n",
    "        self.w1 -= self.lr * w1_grad\n",
    "        self.b1 -= self.lr * b1_grad\n",
    "        # 출력층의 가중치와 절편을 업데이트합니다.\n",
    "        self.w2 -= self.lr * w2_grad\n",
    "        self.b2 -= self.lr * b2_grad\n",
    "        return a\n",
    "   \n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)          # 정방향 계산을 수행합니다.\n",
    "        return np.argmax(z, axis=1)  # 가장 큰 값의 인덱스를 반환합니다.\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        # 예측과 타깃 열 벡터를 비교하여 True의 비율을 반환합니다.\n",
    "        return np.mean(self.predict(x) == np.argmax(y, axis=1))\n",
    "\n",
    "    def reg_loss(self):\n",
    "        # 은닉층과 출력층의 가중치에 규제를 적용합니다.\n",
    "        return self.l1 * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2))) + \\\n",
    "               self.l2 / 2 * (np.sum(self.w1**2) + np.sum(self.w2**2))\n",
    "\n",
    "    def update_val_loss(self, x_val, y_val):\n",
    "        z = self.forpass(x_val)            # 정방향 계산을 수행합니다.\n",
    "        a = self.softmax(z)                # 활성화 함수를 적용합니다.\n",
    "        a = np.clip(a, 1e-10, 1-1e-10)     # 출력 값을 클리핑합니다.\n",
    "        # 크로스 엔트로피 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "        val_loss = np.sum(-y_val*np.log(a))\n",
    "        self.val_losses.append((val_loss + self.reg_loss()) / len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로에서 제공하는 패션MNIST데이터셋을 분류하는 문제를 코드에 적용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_all.shape, y_train_all.shape) # 훈련데이터 60000개, 28 x 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train_all[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_all[:10]) # 타겟데이터의 10개의 인덱스를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['티셔츠/윗도리', '바지', '스웨터', '드레스', '코트', \n",
    "               '샌들', '셔츠', '스니커즈', '가방', '앵클부츠']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names[y_train_all[0]]) # 인간이 보기 편하게 class_names에 넣어서 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_train_all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, \n",
    "                                                  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.bincount(y_train), np.bincount(y_val)\n",
    "\n",
    "# 이미지데이터를 정규화 해준다.(픽셀데이터의 최대값으로 나누기, 0~1사이로 변환) \n",
    "x_train=x_train/255\n",
    "x_val=x_val/255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 784) # flatten , 28*28=784\n",
    "x_val = x_val.reshape(-1, 784) # flatten, 28*28=784\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "타깃을 원-핫인코딩으로 비꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb.fit_transform([0, 1, 3, 1])\n",
    "tf.keras.utils.to_categorical([0, 1, 3]) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = tf.keras.utils.to_categorical(y_train)\n",
    "y_val_encoded = tf.keras.utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_encoded.shape, y_val_encoded.shape) # 타깃값이 10개의 열로 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0], y_train_encoded[0]) # 원핫인코딩 된 것을 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = MultiClassNetwork(units=100, batch_size=256)\n",
    "fc.fit(x_train, y_train_encoded, \n",
    "       x_val=x_val, y_val=y_val_encoded, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fc.losses)\n",
    "plt.plot(fc.val_losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.score(x_val, y_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.permutation(np.arange(12000)%10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_val == np.random.permutation(np.arange(12000)%10)) / 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
